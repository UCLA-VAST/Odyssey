\section{Introduction}
\label{sec:intro}
%\todo{[1-2.5, 1.5 page]/11}

%On conventional hardware platforms such as CPU and GPU, hardware vendors provide high-performance libraries targeting a specific set of applications, e.g., Intel MKL~\cite{MKL} and Nvidia CuDNN~\cite{cudnn}.
%However, such a manual approach is costly and fails to adapt to the rapid evolution of hardware architectures and applications. As a consequence, we have seen an emergence of auto-tuning frameworks for different application domains, such as AutoTVM~\cite{autotvm} and Halide auto-scheduler~\cite{halide_beam}.

%auto-tuning has demonstrated its effectiveness over the traditional library-based approaches~\cite{halide,autotvm,ansor}.
%As an example, the latest auto-tuning framework for TVM~\cite{tvm}, a deep learning compiler, demonstrates that its evolutionary search-based auto-tuner is able to improve the execution time of deep neural networks over the library-based approaches by up to 3.8$\times$ and 1.7$\times$ on Intel CPU and Nvidia GPU, respectively~\cite{ansor}.

%Hardware vendors used to provide high-performance libraries targeting a specific set of applications, e.g., Intel MKL~\cite{MKL} and Nvidia CuDNN~\cite{cudnn}.
%However, this approach is costly and fails to adapt the rapid evolution of hardware architectures and applications. This situation further exacerbates in the domain of deep learning.
%As a consequence, recent deep learning compilation frameworks such as TVM~\cite{tvm} and Halide~\cite{halide} which provide efficient auto-tuning support have attracted a great interest.
%is costly when being applied to new hardware systems and applications given the considerable efforts of manual tuning.
%On conventional hardware platforms such as CPU and GPU,  However, this approach is costly when being applied to new hardware systems and applications given the considerable efforts of manual tuning. This situation is exacerbated in the domain of deep learning where new applications are emerging at an unprecedented speed.
%which poses a significant challenge to the library-based approaches. 
%As a result, recent deep learning compilation frameworks such as TVM~\cite{tvm} and Halide~\cite{halide} which provide efficient auto-tuning support have attracted a great interest.
%from both academia and industry. 
%These frameworks provide expressive DSLs for constructing the design space and leverage machine-learning techniques to scan the design space efficiently. Such an approach has shown its success in terms of both portability and performance. 
Performance optimization for a given class of microarchitectures, also called performance tuning, has long been an important topic given the complexity of hardware systems and applications. 
This issue is intensified on domain-specific architectures (DSA), which grant designers explicit control over the software stack and hardware architecture, opening up a vast design space to explore. 

This paper focuses on the performance tuning of systolic arrays. 
%A systolic array is consisted of a grid of regular processing elements (PE) that are connected via local interconnects. In the recent years, systolic arrays have been widely adopted in accelerating deep learning applications~\cite{xuechao_sa,tpu,aws,chen_communication}.
%We use an open-source FPGA-based systolic array compiler, AutoSA~\cite{autosa}, for constructing the design space. AutoSA represents the state-of-the-art effort in automatic systolic array synthesis. It generates systolic arrays with comparable or better performance than the previous works~\cite{autosa}. This is achieved by incorporating a comprehensive set of hardware optimization techniques such as space-time mapping, array partitioning, latency hiding, SIMD vectorization, and double buffering. However, the rich set of optimization passes in AutoSA also pose a great challenge to performance tuning. When considering all the available tuning options offered by the compiler, the design space bloats to a size of $O(2^{40})$ for a $1024\times1024\times1024$ matrix multiplication (MM). Such a vast design space is infeasible to explore manually, underscoring the strong needs for efficient auto-tuning approaches.
%Figure \todo{...} presents a 2D systolic array for matrix multiplication, which can be generated by an open-source FPGA-based systolic array compiler~\cite{autosa}. A complete systolic array is composed of two parts: PEs and the on-chip I/O network for transferring data between PEs and the external memory. Each component needs take carefully optimized to achieve high performance. Figure \todo{...} shows the 
A complete design space of systolic arrays contains multiple dimensions, such as the selection of dataflows, loop permutation and tiling.
% \footnote{Given a target loop program to map to systolic arrays, designers could select different loops to be mapped to spatial dimensions of systolic arrays, leading to different array topologies and execution models. We define each of such choices as a unique dataflow.}
%, loop tiling and permutation. selecting different loops to map to the spatial dimensions of the arrays, applying loop tiling with different tiling factors, and permuting any loops to achieve better performance.
These factors impact the final design performance in an intertwined manner and compose a vast design space which is intractable to explore exhaustively, especially for large problem sizes.
%explore, underscoring the strong needs for efficient auto-tuning approaches.
%These factors impact the design performance in an intertwined manner, and need to carefully tuned to achieve optimal performance. 

%A complete auto-tuning problem of systolic arrays contains two dimensions: the \textit{construction} of the design space and the implementation of the \textit{search method}. An incomplete design space may leave out optimal designs and lead to inferior performance, while a more comprehensive design space brings a higher complexity and poses greater challenges to the search methods. 

Many previous works have attempted this challenging task by looking into different dimensions of the design space and proposing various auto-tuning methods~\cite{timeloop,confuciux,cosa,gamma,dmazerunner,marvel,tenet,interstellar}. However, after a thorough examination of the previous works, we identified several limitations that need to be addressed. 
% gamma

% \begin{figure}[t]
%     \centering
%     \subfloat[Throughput]{
%         \includegraphics[width=0.45\columnwidth]{figs/mm_dataflow_throughput_%divisor.png}
%         \label{fig:tuning_dataflow_mm_latency_divisor}
%     }
%     \subfloat[DSP]{
%         \includegraphics[width=0.45\columnwidth]{figs/mm_dataflow_dsp_divisor%.png}
%         \label{fig:tuning_dataflow_mm_latency_dsp_divisor}
%     }
%     \caption{Throughput and DSP usage of different designs for MM considering %only divisor tiling factors.}
%     \label{fig:tuning_mm_cmp_divisor}
% \end{figure}

% \todo{add figures summarizing the performance gap.}
% \begin{figure}[t]
%     \centering
%     %\subfloat[Throughput]{
%     \begin{subfigure}{0.45\columnwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/motiv_throuhgput.png}
%         \caption{Throughput}
%     \end{subfigure}
%     %\subfloat[DSP]{
%     \begin{subfigure}{0.45\columnwidth}
%         \includegraphics[width=\linewidth]{figs/motiv_dsp.png}
%         \caption{DSP}
%         \label{fig:motive_dsp}
%     \end{subfigure}
%     \vspace{-0.05in}
%     \caption{Throughput and DSP usage of different systolic array designs for a 1024x1024x1024 MM. Design 4 is produced by Odyssey.}
%     \vspace{-0.05in}
%     \label{fig:motiv_throughput}
%     %\label{fig:tuning_motiv}
% \end{figure}

\textbf{Limitation 1: Incomplete coverage of the design space.} 
%Loop tiling is an important technique for performance optimization. 
When selecting the tiling factors, many previous works only considered problem size divisors to reduce the design space~\cite{dmazerunner,interstellar,marvel,cosa}. 
However, such a simplification could lead to inferior designs. We compare the throughput and DSP usage of best systolic arrays found when limiting tiling factors to 1) divisors only and 2) both divisors and non-divisors for a $1024\times 1024\times 1024$ matrix multiplication (MM)\footnote{Please refer to Section~\ref{sec:evo_details} for more details.}. Limiting to divisors leads to a 39\% performance loss. With the limited design space, the divisor-only design fails to fully exploit the on-chip resource, with only 60\% DSP usage.

\textbf{Limitation 2: Inaccurate performance modeling.} An inaccurate performance model could also hurt the quality of search results. For example, the previous work TENET~\cite{tenet} estimated the design latency as the maximum of compute and communication latency. This model overlooks the prologue/epilogue phases when loading the first tile of data and writing out the final results. For the same MM problem, the best design found by using the simplified maximum-based performance model results in 9\% slower design than the best design found by a more accurate performance model that accounts for prologue/epilogue latencies.

\textbf{Limitation 3: Inefficient search methods and imperfect pruning heuristics.} %Table~\ref{table:prior_work_tuning} lists several recent works for performance tuning of systolic arrays. 
When searching the design space, many previous works adopted pruning-based exhaustive search which may not scale to large-sized problems~\cite{dmazerunner,interstellar,marvel,tenet}. To make matters worse, several works chose imperfect pruning heuristics, failing to cover optimal designs. For example, the previous work Marvel~\cite{marvel} pruned the design space based on the off-chip data communication and applied an exhaustive search in the pruned sub space. However, an optimal design needs to balance both the data communication and computation and does not necessarily minimize the off-chip memory accesses. We found that the design with the highest performance has a $3.5\times$ more off-chip data movement and a $1.9\times$ higher throughput than the design with minimum off-chip data movement. 
% As a result, the best design found which minimizes the off-chip data communication results in a 45\% performance loss compared to the optimal design.

%. We show in this paper that the best systolic array design does not necessarily minimize the off-chip data communication. For the same MM problem, the design that minimized the off-chip data communication is 2$\times$ slower than the optimal design. 
%found that takes both communication and computation into consideration.
%For example, previous work dMazeRunner~\cite{dmazerunner} and Marvel~\cite{marvel} chose to prune the designs based on the computation resource utilization. The pruning threshold should be adjusted dynamically according to the problem sizes. On Xilinx Alveo U250 FPGA, the best design for a $1024\times1024\times1024$ matrix multiplication (MM) uses 98\% of the total DSP resource. However, this number is decreased to only 14\% for a $64\times64\times64$ MM given the limited parallelism.
%Note that some of the listed works in the table cover a broader domain of architectures beyond systolic arrays. In this work, we only focus on the systolic array architecture. In the rest of this section, we discuss the prior works from two dimensions: the construction of the design space and the search methods.

All of these limitations affect the quality of search results and further impact the architecture decisions that designers derive based on these results. 
%As an example, Figure \todo{...} presents the performance of best systolic arrays found with different dataflows for the MM example when using only prime factors and taking non-prime factors into account. When limiting tiling factors to only prime factors, designers might draw the conclusion that dataflow plays a minimal role in determining the performance as 5 out of the 6 total different dataflows achieve the similar performance. However, after taking non-prime factors into consideration, a different conclusion is derived that the dataflow that maps loop $i$ and $j$ to space dimensions (annotated as $[i,j]$, which will be explained in detail later) dominates the performance.
%effectiveness of the auto-tuning methods and the quality of search results. 
To overcome this challenge, in this paper, we propose a new automatic design space exploration framework for systolic arrays, \textit{Odyssey}\footnote{Odyssey is abbreviated from \underline{AU}tomatic \underline{DE}sign space exploration for \underline{SY}stolic arrays.}, with the following contributions:
% Figure~\ref{fig:tuning_flow} depicts the proposed tuning flow.

% \begin{figure}[t]
% \includegraphics[width=0.75\columnwidth]{figs/odyssey_flow.pdf}
% \centering
% \vspace{-0.05in}
% \caption{Overview of Odyssey framework.}
% \vspace{-0.05in}
% \label{fig:tuning_flow}
% \end{figure}

% Odyssey leverages AutoSA~\cite{autosa}, an open-source FPGA-based systolic array compiler, to construct the design space automatically. 
% AutoSA takes in a C program that describes the target algorithm to map to systolic arrays and generates the systolic array designs in Xilinx HLS C~\cite{xilinx_hls}.
% %\href{https://github.com/UCLA-VAST/AutoSA}{open-source}
% %and applies a sequence of hardware optimizations based on the polyhedral framework~\cite{isl} to generate the hardware designs in Xilinx HLS C~\cite{xilinx_hls}. 
% We extend the AutoSA framework to generate a design description file that covers the full details of the generated hardware. 
% Odyssey uses this file to create hardware performance models as symbolic expressions of the tuning parameters that can be used by the auto-tuner.
% %Odyssey then builds Python functions for estimating the design performance including the latency and resource usage from the description file. The performance model is plugged into the auto-tuner to search for the optimal design.
% Inside the auto-tuner, Odyssey implements a two-stage flow that starts with a mathematical programming (MP)-based optimizer that leverages optimization solvers with a simplified objective function to produce an initial high-quality design, followed by the evolutionary search with the accurate performance models. Odyssey surpasses the previous works in multiple dimensions. 
% The contributions of this work are:
\begin{enumerate}
    \item A comprehensive design space construction and accurate performance modeling for systolic arrays
    \item	Two design space explorers:
    \begin{enumerate}
        \item A hybrid method using genetic algorithm and mathematical programming
        \item A more accurate method using a novel padding-based search algorithm
    \end{enumerate}	
    \item A fully automated and open-source framework
\end{enumerate}
% \textbf{Contribution 1: A comprehensive design space and accurate performance modeling.} 
% %First, Odyssey covers a comprehensive design space with accurate performance modeling. 
% %Previous works ~\cite{marvel,cosa,tenet,maestro,interstellar} have shown that different design factors including the choices of dataflows, loop permutation and tiling affect the performance of systolic arrays in an intertwined manner. All of these design factors can be explored by AutoSA and are taken into account by the Odyssey framework. 
% Odyssey covers a comprehensive design space including the selection of dataflows, loop permutation and tiling. Furthermore, Odyssey derives the performance models directly from the compiler-generated hardware with high estimation accuracy.
% %, in contrast to previous works that built analytical models based on the hypothetical architectures. 
% For example, we are able to produce latency models by traversing the ASTs of the generated HLS designs, which achieved a low estimation error of 1.99\% compared to the real hardware execution.

% \textbf{Contribution 2: Efficient auto-tuning methods.} The proposed auto-tuning algorithm does not rely on any artificial assumptions (e.g., divisors tiling factors, pruning based on off-chip data communication) to limit the design space, and yet it is highly efficient with a hybrid optimization technique that combines the MP-based optimization and evolutionary search. 
% %It uses the evolutionary search as the backbone and leverages the solver-based analysis for generating high-quality seeds to reduce the convergence time. 
% We propose a hybrid mutation operation in evolutionary search that takes non-divisor tiling factors into consideration. In addition, we implement a pruning algorithm for loop permutation that trims away inferior designs without omitting the optimal designs.
% As a result, our search framework can locate designs with high performance in a short amount of time, For example, for the $1024\times1024\times1024$ MM, the proposed auto-tuner finds a design that achieves 90\% of the optimal performance in 5 seconds with a single CPU thread.

% \textbf{Contribution 3: A fully automated and open-source framework.} 
% The entire flow is fully automated and open-sourced\footnote{\url{https://github.com/UCLA-VAST/AutoSA/tree/master/autosa_scripts/odyssey}}. Odyssey is the first work that is built directly on an open-source systolic array compiler to construct the design space and generate performance models based on real hardware implementations. This guarantees the comprehensiveness and accuracy of the design space modeling, and validity and reproducibility of the search results.
% %We use AutoSA to generate all possible architecture candidates and derive the hardware models for auto-tuning without any user intervention. 
% In this paper, we show two architecture studies on MM and convolutionary neural network (CNN). 
% %The open-source hardware generator guarantees the validness and reproducibility of the search results. 
% %We will release the Odyssey framework upon the publication of this paper.
% %In comparison, several prior works~\cite{dmazerunner,maestro,tenet} require designers to explicitly describe the hardware models which takes non-trivial efforts and is error-prone.

% %In addition to the comprehensive and accurate design space modeling, we propose a highly efficient auto-tuning method to explore this design space. 

% The rest of this paper is organized as follows: Section~\ref{sec:background} discusses the previous works and covers the basics of AutoSA.
% %Section~\ref{sec:scope} covers the scope of Odyssey framework. 
% Section~\ref{sec:construct} explains how we construct the design space. In Section~\ref{sec:scan}, we introduce the search methods to explore this design space. Section~\ref{sec:evaluate_results} presents the evaluation results. Section~\ref{sec:discuss} discusses lessons learned from this work and Section~\ref{sec:limit} states the limitations of this work.
% Section~\ref{sec:conclude} concludes the paper.

% %This document provides instructions for submitting papers to the 27th
% %International Conference on Architectural Support for Programming
% %Languages and Operating Systems (ASPLOS), 2022.  In an effort to
% %respect the efforts of reviewers and in the interest of fairness to
% %all prospective authors, we request that all submissions to ASPLOS
% %2022 follow the formatting and submission rules detailed below.
% %Submissions that violate these instructions may not be reviewed, at the
% %discretion of the program co-chairs.
% %
% %The submission instructions are also available in
% %\href{https://asplos-conference.org/submissions/}{this website},
% %including a link to the paper submission site. The website contains
% %sample PDF files for the paper
% %and
% %extended abstract. The
% %sample files are formatted using the ASPLOS'22 submission format and
% %contain the submission and formatting guidelines. The website also
% %includes an archive file
% %with \LaTeX~templates for both papers and extended abstracts.
% %
% %All questions regarding paper formatting and submission should be directed
% %to the program co-chairs.
% %
% %\paragraph{Important highlights:}
% %\begin{itemize}
% %\item Papers should contain a
% %maximum of 11 pages of single-spaced two-column text but not including
% %references.
% %\item All submitted papers must be accompanied by an extended
% %  abstract, in a separate file with a maximum of 
% %2 pages of single-spaced two-column text, not counting references.
% %\item Papers and extended abstracts must be submitted in printable PDF format.
% %\item Text must be in a minimum 10pt ({\bf not} 9pt) font.
% %\item No page limit for references for papers and the extended abstracts.
% %\item Each reference must specify {\em all} authors (no {\em et al.}).
% %\item Proceedings will appear in the ACM digital library up to two weeks
% %before the conference.
% %\end{itemize}

% %\section{Paper and Abstract Preparation Instructions}
% %
% %\subsection{Paper Formatting}
% %
% %Papers must be submitted in printable PDF format and should contain a
% %maximum of {\bf 11 pages} of single-spaced two-column text, not counting
% %references. The paper may include any number of pages for
% %references, but see below for more instructions. If you are using
% %\LaTeX~\cite{lamport94} to typeset your paper, then we suggest that
% %you use the template that we provide online.
% %If you use a different
% %software package to typeset your paper, then please adhere to the
% %guidelines given in Table~\ref{table:formatting}.
% %
% %\begin{table}[h!]
% %  \centering
% %  \begin{tabular}{|l|l|}
% %    \hline
% %    \textbf{Field} & \textbf{Value}\\
% %    \hline
% %    \hline
% %    File format & PDF \\
% %    \hline
% %    Page limit & 11 pages, {\bf not including}\\
% %               & {\bf references}\\
% %    \hline
% %    Paper size & US Letter 8.5in $\times$ 11in\\
% %    \hline
% %    Top margin & 1in\\
% %    \hline
% %    Bottom margin & 1in\\
% %    \hline
% %    Left margin & 0.75in\\
% %    \hline
% %    Right margin & 0.75in\\
% %    \hline
% %    Body & 2-column, single-spaced\\
% %    \hline
% %    Separation between columns & 0.25in\\
% %    \hline
% %    Body font & 10pt\\
% %    \hline
% %    Abstract font & 10pt, italicized\\
% %    \hline
% %    Section heading font & 12pt, bold\\
% %    \hline
% %    Subsection heading font & 10pt, bold\\
% %    \hline
% %    Caption font & 9pt, bold\\
% %    \hline
% %    References & 8pt, no page limit, list \\
% %               & all authors' names\\
% %    \hline
% %  \end{tabular}
% %  \caption{Formatting guidelines for submission. }
% %  \label{table:formatting}
% %\end{table}
% %
% %\textbf{Please ensure that you include page numbers with your
% %submission}. This makes it easier for the reviewers to refer to different
% %parts of your paper when they provide comments.
% %
% %Please ensure that your submission has a banner at the top of the title
% %page, as shown in
% %this
% %sample paper, which contains the submission number and the notice of
% %confidentiality.  If using the template, just replace XXX with your
% %submission number.
% %
% %\subsection{Extended Abstract Formatting}
% %
% %The extended abstracts must be submitted in printable PDF format and should contain a
% %{\bf maximum of 2 pages} of single-spaced two-column text, {\bf not
% %  counting references}.  You may include any number of pages for
% %references, but see below for more instructions. The extended
% %abstracts should use the same formatting as the papers. If you are using
% %\LaTeX~\cite{lamport94} to typeset your extended abstract, then we suggest that
% %you use
% %our
% %  template that also describes what information to include in your
% %extended abstract. 
% %
% %The extended abstract and the paper must be independent (standalone) documents that a reviewer can read %separately, as some rejection decisions may be made based only on extended abstracts. The extended abstract %can refer to figures and sections in the main paper.
% %
% %You can check the extended abstracts of papers published at ASPLOS 2021 
% %\href{https://asplos-conference.org/2021/index.html%3Fp=2181.html}
% %{here}.
% %
% %\subsection{Content}
% %
% %\noindent\textbf{Anonymity.}  Reviewing will be \textbf{double blind};
% %therefore, please \textbf{do not include any author names on any submitted
% %documents} except in the space provided on the submission form.  
% %
% %Pay attention not to reveal the author or affiliation information through
% %side channels:
% %
% %\begin{itemize}
% %\item The metadata included in the PDF should not give away such
% %information. 
% %
% %\item If you are improving upon your prior work, refer to your prior
% %work in the third person and include a full citation for the work in the
% %bibliography.  For example, if you are building on {\em your own} prior
% %work in the papers \cite{nicepaper1,nicepaper2,nicepaper3}, you would say
% %something like: "While the authors of
% %\cite{nicepaper1,nicepaper2,nicepaper3} did X, Y, and Z, this paper
% %additionally does W, and is therefore much better."  Do NOT omit or
% %anonymize references for blind review, unless
% %your own prior work appeared in IEEE CAL or workshops without archived
% %proceedings, as discussed later in this document.
% %
% %\item If your system is already released to the public, please rename your system in your submission.
% %
% %\item You should avoid revealing affiliation (e.g., by identifying your company’s name) in your paper. %Instead, please use a generic name, like “a cloud service provider X”.  
% %If concealing system name or affiliation would make your paper difficult to understand, contact the program %chairs to discuss exceptions to this policy.
% %\end{itemize}
% %
% %(For more frequently-asked questions about double-blind reviewing, please 
% %consult this 
% %\href{https://pldi20.sigplan.org/track/pldi-2020-papers#FAQ-on-Double-Blind-Reviewing}
% %{FAQ from PLDI 2020}.)
% %
% %\textbf{Violating the above anonymity requirement will be rejected without review}. If you have any %concerns, please contact the program chairs before your paper submission.
% %
% %\noindent\textbf{Figures and Tables.} Ensure that the figures and tables
% %are legible. 
% %Reviewers may print the papers in gray-scale. Therefore, if you use
% %colors for your figures, ensure that the different colors are highly
% %distinguishable in gray-scale.
% %
% %\noindent\textbf{References.}  There is no length limit for references.
% %Each reference must explicitly list all authors of the paper.
% %Knowing all authors of related
% %work will help find the best reviewers. Since there is no length limit
% %for the number of pages used for references, there is no need to save space
% %here.
% %
% %\section{Paper and Abstract Submission Instructions}
% %
% %\subsection{Declaring Authors}
% %
% %Declare all the authors of the paper up front. Addition/removal of authors
% %once the paper is accepted will have to be approved by the program co-chairs,
% %since it potentially undermines the goal of eliminating conflicts for
% %reviewer assignment.
% %
% %\subsection{Areas and Topics}
% %
% %ASPLOS emphasizes multidisciplinary research. Submissions should ideally
% %emphasize synergy of two or more ASPLOS areas: architecture, programming
% %languages, operating systems, and related areas (broadly
% %interpreted). Authors should indicate these areas on the submission form.
% %
% %Authors should also indicate \textbf{at most 4}
% %topics covered by the paper on the submission form 
% %for optimal reviewer match. If more than 4 topics are selected, some topics
% %will be randomly dropped by program chairs.
% %
% %If
% %you are unsure whether your paper falls within the scope of ASPLOS, please
% %check with the program co-chair -- ASPLOS is a broad, multidisciplinary
% %conference and encourages new topics.
% %
% %\subsection{Declaring Conflicts of Interest}
% %
% %Authors must register all their conflicts on the paper submission site.
% %Conflicts are needed to ensure appropriate assignment of reviewers.
% %If a paper is found to have an undeclared conflict that causes
% %a problem OR if a paper is found to declare false conflicts in order to
% %abuse or ``game'' the review system, the paper may be rejected.
% %
% %Please declare a conflict of interest (COI) with the following people
% %for any author of your paper:
% %
% %\begin{enumerate}
% %\item Your Ph.D. advisor(s), post-doctoral advisor(s), Ph.D. students,
% %      and post-doctoral advisees, forever.
% %\item Family relations by blood or marriage and close personal friends, forever (if they might be potential %reviewers).
% %\item People with whom you have collaborated in the last four years, including
% %\begin{itemize}
% %\item co-authors of accepted/rejected/pending papers.
% %\item co-PIs on accepted/rejected/pending grant proposals.
% %\end{itemize}
% %\item People (including students) who shared your primary institution(s) in the
% %last four years.
% %\end{enumerate}
% %
% %You need not and should not declare a COI for the following cases:
% %
% %\begin{itemize}
% %\item ``Service'' collaborations such as co-authoring a report for a professional
% %organization or an open-source community, serving on a program committee, or co-presenting
% %tutorials, do not themselves create a conflict of interest.
% %
% %\item Co-authoring a paper that is a compendium of various projects without
% %direct collaboration among the projects does not constitute a
% %conflict among the authors of the different projects.
% %
% %\item Internships constitute a conflict of interest during the period of employment of the intern, but not %thereafter, unless some other provision applies (e.g., coauthorship or ongoing research collaboration after %the internship).
% %
% %\item You \textbf{must not} declare a COI with a reviewer just because that reviewer works on topics %similar to or related to those in your paper. 
% %
% %\end{itemize}
% %
% %Please declare all your conflicts, not just restricted to the PC and ERC, as we may occasionally ask for %reviews from people outside the PC and the ERC. 
% %
% %When in doubt, contact the program chairs.
% %
% %\subsection{Concurrent Submissions and Workshops}
% %
% %By submitting a manuscript to ASPLOS'22, the authors guarantee that the
% %manuscript has not been previously published or accepted for publication in
% %a substantially similar form in any conference, journal, or workshop. The
% %only exceptions are (1) workshops without archived proceedings such as in
% %the ACM digital library (or where the authors chose not to have their paper
% %appear in the archived proceedings), or (2) venues, such as IEEE CAL, where
% %there is an explicit policy that such publication does not preclude longer
% %conference submissions. These are not considered prior publications. 
% %Technical reports and papers posted on public social media sites, Web pages,
% %or online repositories, such as arxiv.org, are not considered prior
% %publications either. In these cases, the submitted manuscript may
% %ignore the posted work to preserve author anonymity. 
% %The authors also guarantee that no paper that contains
% %significant overlap with the contributions of the submitted paper will be
% %under review for any other conference, journal, or workshop during the
% %ASPLOS'22 review period. Violation of any of these conditions will lead to
% %rejection. As always, if you are in doubt, it is best to contact the
% %program co-chairs.  Finally, we also note that the ACM Plagiarism Policy
% %(http://www.acm.org/publications/policies/plagiarism\_policy) covers a range
% %of ethical issues concerning the misrepresentation of other works or one's
% %own work.
% %
% %\subsection{Ethical Obligations}
% %\begin{itemize}
% %\item Authors are not allowed to contact reviewers or PC members to encourage or solicit them to bid on any %paper.
% %\item Authors are not allowed to attempt to sway a reviewer to review any paper positively or negatively.
% %\item Authors are not allowed to contact reviewers or PC members requesting any type of information about %the reviewing process, either in general or specifically about submitted papers.
% %\item Authors are not allowed to contact reviewers or PC members to ask about the outcomes of any papers.
% %\item Authors must also abide by the
% %  \href{https://www.acm.org/code-of-ethics}{ACM ethics
% %    policy}. Violation of the ACM ethics policy may result in
% %  rejection of the submission and possible action by the ACM.
% % \item Authors are not allowed to advertise their submissions or related technical reports and postings %(e.g., to arxiv.org or online repositories) on social media or community blogs and webpages during the %period starting two weeks before the submission deadline and ending when the ASPLOS’22 acceptance results %are public.
% %\end{itemize}
% %
% %\section{Early Access in the Digital Library}
% %
% %The ASPLOS'22 proceedings will be freely available via the ACM Digital
% %Library for up to two weeks before the
% %conference. Authors must consider any implications of this early
% %disclosure of their work {\em before} submitting their papers.%
\section{Searching the Design Space}
\subsection{Previous Search Methods}
\label{sec:prev_search_method}
The vast design space makes it infeasible to explore with exhaustive search. Prior works have proposed various approaches to tackle this challenge, as summarized in Table~\ref{table:prior_work_tuning}. 
We classify the search methods into three categories.
%: brute-force methods, analytical modeling-based methods, and iterative methods. 

\subsubsection{Brute-force methods} Such methods include random search~\cite{timeloop,autotvm} and exhaustive search with pruning~\cite{autosa,dmazerunner,interstellar,marvel,tenet}. Brute-force methods face scalibility issues with large-scale problems.
% xuechao_sa
%These methods are sufficient for designs with a relatively small design space. However, the converge time stretches significantly for large problem sizes due to the low sample efficiency. 
For pruning-based exhaustive search, imperfect pruning heuristics could lead to inferior designs. 
%Another commonly overlooked problem for pruning-based exhaustive search is the choice of pruning heuristics. 
In Section~\ref{sec:intro}, we mentioned using off-chip data communication as the pruning heuristic leads to inferior performance. Previous works like~\cite{marvel,chen_communication} used this heuristic.
%As an another example, several prior works used resource utilization ratio as the pruning heuristic~\cite{marvel,dmazerunner,xuechao_sa}. However, this heuristic needs to be adjusted dynamically to avoid over-pruning the profitable designs. 
%For instance, on Xilinx Alveo U250 FPGA, the best systolic array design for a $64\times64\times64$ MM only takes 14\% of the total DSP resource given the limited parallelism. When increasing the matrix dimension from 64 to 1024, the best systolic array uses 98\% of the total DSP resource. Using a fixed pruning threshold will prune away all the profitable designs for smaller problem sizes. Therefore, designers need to take non-trivial efforts in profiling the design space and select the proper pruning threshold. 
%Previous work dMazeRunner~\cite{dmazerunner} chose 60\% as the utilization pruning threshold. Using this number will prune away all the feasible designs for the small MM problem as presented above. 
%However, the process of choosing an ideal pruning heuristic requires non-trivial efforts as programmers are usually faced with an unknown design space without any prior knowledge of the performance distribution of the feasible designs. 
%Taking into account of all the challenges above, brute-force approaches are limited in practice when handling complex hardware systems and applications.

\subsubsection{Mathematical programming-based methods} These methods formulate the search task as a mathematical optimization problem and resolve it with optimization solvers~\cite{analytical_cnn,analytical_tc,marvel,cosa}. 
%This method can converge to a solution in seconds, however, it usually yields sub-optimal search results due to the over-simplification of the design space. 
The recent work CoSA~\cite{cosa} modeled the design space as a mixed integer programming (MIP) problem. To accommodate for the formulation requirements of MIP, CoSA set the objective function as a linear combination of several high-order factors such as resource utilization and data communication.
%used an inaccurate performance model with many simplifications. 
%including dataflows, loop permutation and tiling as an mixed integer programming (MIP) problem. However, to accommodate for the stringent formulation requirements of MIP, CoSA used an inaccurate performance model with many simplifications. 
%For example, the memory usage is computed as the product of all the tiling factors which overlooks the banking property of SRAMs. 
The simplified models employed in such approaches will lead to inferior designs. We conducted an experiment in which we model the tuning task for AutoSA-generated designs as a non-linear optimization problem and used the off-the-shelf solver (AMPL~\cite{ampl} with Ipopt~\cite{ipopt}) to generate the solutions. The best design obtained from this approach is 1.5$\times$ slower than the optimal design obtained by Odyssey (see Section~\ref{sec:solver_details}). 
\vspace{-0.1in}
\subsubsection{Iterative methods} 
%Iterative-based tuning methods reach a sweet spot between the first two categories. 
%It requires minimal prior knowledge of the design space, making it easier to adapt to different search problems. It achieves a higher sample efficiency with the feedback-based exploration strategy compared to the brute-force approaches. Furthermore, programmers can supply it with an accurate and detailed performance model or evaluate the design point on the hardware directly, which helps cover the optimal designs with high performance compared to the solver-based methods. 
Examples include simulated annealing~\cite{autotvm}, evolutionary search~\cite{gamma}, Bayesian optimization~\cite{david_bayesian}, and reinforcement learning~\cite{confuciux}. In this work, we examined several iterative search methods and chose evolutionary search given its high sample efficiency and search quality.

%However, iterative tuning methods are usually involved with many hyper-parameters which need to be carefully adjusted to achieve the best performance. This additional complexity leads to trade-offs in choosing different tuning methods for various search problems.  

%As shown in Table~\ref{table:prior_work_tuning}, most previous works only covered part of the design space, and relied on brute-force methods such as exhaustive search with pruning to explore the design space. 
%The recent work ConfuciuX~\cite{confuciux} proposed a search method based on reinforcement learning and evolutionary search to explore the design space efficiently. However, the search space is highly limited as they only considered choosing the memory size and PE numbers in a fixed parameter set. 
An ideal performance tuning framework should achieve: 1) a \textit{comprehensive coverage} and \textit{accurate modeling} of the design space, and 2) \textit{efficient} search methods to explore the design space. The failure in either of the two targets will impact the quality of search results, as well as the architecture conclusions derived from these results. Unfortunately, regardless of the plethora of past studies, we observe no prior work that reached a balance between these two goals. This situation has motivated us to tackle this challenge. %Specifically, the goal in this work is: \textit{Develop an efficient design space exploration framework that covers a comprehensive and accurate design space for systolic arrays}.

\subsection{Genetic-MP Hybrid Method}
\label{sec:evo_details}
\subsubsection{Genetic algorithm}
For Odyssey's first explorer, we select evolutionary search as the backbone search method. 
%Previous works on architecture exploration have implemented various search methods, as summarized in Section~\ref{sec:prev_search_method}. We evaluated different methods on our search problem and found that evolutionary search achieved the best search performance. Detailed comparison results are covered in Section~\ref{sec:search_method_cmp}. We present the implementation details of evolutionary search in this section.
Evolutionary search~\cite{genetic} is a generic meta-heuristic algorithm inspired by biological evolution, in which individuals of a population gradually improve themselves through a series of biological mechanisms such as \textit{mutation}, \textit{crossover}, and \textit{selection}. 
In the context of hardware design space exploration, we have:
% each design point is termed as an individual in the population. The feature vector that describes a design point is coined as the genome of the individual. Each design point is assigned a fitness score by its performance assessed by the real hardware measurement or the performance model. At each iteration, we select a group of individuals with the highest fitness scores in the population as the parents to produce the children in the next iteration (\textit{selection}). These parents will breed new individuals with \textit{mutation} and \textit{crossover} operations. In the next iteration, we filter out the low-fitted individuals and repeat the same reproduction process until finding the satisfactory solutions. 

\textbf{Encoding scheme: }
%AutoSA generates designs with different dataflows and loop permutations, and leaves the tiling factors to the auto-tuner to explore. 
Each individual is encoded by the tiling factors used in AutoSA compilation passes. 
%Figure~\ref{fig:evo_mutation} shows one example of the MM program.
The encoded genome includes the problem size and the tiling factors used for each compilation pass. 
%At present, we leave the communication management to the compiler to be handled automatically.

\textbf{Mutation: }
We randomly select one loop $l_2$ and mutate this loop by changing the loop bound to a random value $s\in [1,l_2]$. Next, we select another corresponding loop $l_1$ and change its loop bound to a new value $s'$ computed by $s'=ceil(l_1\times l_2/s)$.
We show one example of mutation in Figure~\ref{fig:evo_mutation} where the new tiling factor $T_{j1}=36$ is not a divisor of 64.
% We first compute the updated loop bounds with the current tiling factors. The mutation process operates on the values of the loop bounds. We consider non-divisor tiling factors by randomly choosing one loop $l_1$, dividing it by a random divisor $\alpha$ ($\alpha|l_1$), and multiplying $\alpha$ to another loop $l_2$ that is derived from the same loop as $l_1$. Figure~\ref{fig:evo_mutation} depicts an example of two mutation methods.
% We implement two mutation methods: \textit{factorization-based} mutation and \textit{random} mutation. 

\begin{figure}[h]
\includegraphics[width=0.9\columnwidth]{figs/mutation.pdf}
\centering
\caption{Example of a MM genome mutation.}
\label{fig:evo_mutation}
\vspace{-0.2in}
\end{figure}

% \textit{1) Factorization-based mutation:} We randomly choose one loop $l_1$, divide it by a random divisor $\alpha$ ($\alpha|l_1$), and multiply $\alpha$ to another loop $l_2$ that is derived from the same loop as $l_1$. For example, in Figure~\ref{fig:evo_mutation}, we select the loop $k.2$ and divide it by 2, then we choose the other loop $k.1$ and multiply its current value with 2. The factorization-based mutation keeps the product of the tiled loop sizes unchanged, guaranteeing the muted program is always valid.

% \begin{figure}[t]
% \includegraphics[width=\columnwidth]{figs/mutation_cmp.png}
% \centering
% \caption{Performance comparison of two mutation methods. (One epoch is defined by %evaluating one design sample (individual) in evolutionary search.)}
% \label{fig:evo_mutation_cmp}
% \end{figure}

% The factorization-based mutation always chooses the divisors of the loop bounds. While such an implementation is common in many previous works~\cite{cosa,timeloop,dmazerunner,interstellar}, it could result in a reduced design space with inferior performance. 
% %In Figure~\ref{fig:evo_mutation_cmp}, we compare two different tuning configurations using our auto-tuner, one with only the factorization-based mutation, the other with the hybrid mutation methods which will be introduced soon that considers the non-divisor tiling factors. We search the latency-optimal systolic arrays for a $1024\times1024\times1024$ MM. The y-axis is the normalized throughput (1/latency) against the highest throughput achieved by these two configurations. As shown in the figure, the best design found when only considering the divisors has a 35\% performance gap compared to the best design found by the hybrid method where non-divisors are considered. 
% Table~\ref{table:tuning_mutation_cmp} compares the best systolic arrays identified by two search methods for a $1024\times1024\times1024$ MM. The first method uses the factorization-based mutation that only considers divisor tiling factors. The second method employs the hybrid mutation method which we will introduce soon that considers non-divisor tiling factors. As shown in the table, the first design found with the factorization-based mutation has a 39\% performance gap compared to the second design. The use of non-divisor tiling factors (e.g., $T\_{I1}=129$, $T\_{J1}=130$) helps locate a better design that fully utilizes the DSPs, while the divisor-only design only uses 60\% of the DSPs due to the limited choices of tiling factors. This result reveals the necessity of taking non-divisors into consideration to achieve high performance. To accommodate for the non-divisor tiling factors, we introduce the second mutation scheme, \textit{random mutation}. 

% \begin{table}[t]
% \centering
% \caption{Best solutions found by two mutation methods.}
% \vspace{-0.05in}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lccccccccc}
% \toprule
% Mutation Methods         & Throughput      & BRAM & DSP  & T\_{I1} & T\_{J1} & T\_{K1} & T\_{I2} & T\_{J2} & T\_{K2} \\ \toprule
% Factorization   & 0.61$\times$ & 48\%    & 60\% & 64   & 128   & 128    & 16     & 4     & 8   \\ \hline
% Factorization + Random  & 1.00$\times$ & 99\%    & 100\% & 129   & 130   & 64    & 3     & 13     & 4   \\ \bottomrule
% \end{tabular}
% }
% \label{table:tuning_mutation_cmp}
% \vspace{-0.05in}
% \end{table}



% \textit{2) Random mutation:} We randomly select one loop $l_1$ and mutate this loop by changing the loop bound to a random value $s\in [1,l_1]$. Next, we select another corresponding loop $l_2$ and change its loop bound to a new value $s'$ computed by $s'=ceil(l_1\times l_2/s)$.

% We show one example of random mutation in Figure~\ref{fig:evo_mutation}. In the right example, we select the loop $j.2$ and change its original loop size from 8 to 6. Then we choose the other loop $j.1$ to mutate. We compute the new loop bound for $j.1$ as $ceil(4\times8/6)=6$. As a result, the tiling factor $T\_J1$ is changed from 32 to 36, which is a non-divisor of the problem size $J=64$. The random mutation method adjusts two loops at the same time as an effort to keep the product of these two loops with minimal changes. The use of \texttt{ceil()} function guarantees the new product is no less than the original product so that the mutated program is always legal.

% \begin{figure}[t]
% \includegraphics[width=\columnwidth]{figs/alpha_tuning.png}
% \centering
% \caption{Tuning performance comparison with different hyper-parameter $\alpha$.}
% \label{fig:evo_mutation_alpha}
% \end{figure}

When performing the mutation, we assign a probability $\alpha$ to execute the factorization-based mutation and the probability $1-\alpha$ to the random mutation. Based on a grid search, we set $\alpha$ to 0.4 by default.
%to find the best parameter $\alpha$ that maximizes the search performance. 
%Figure~\ref{fig:evo_mutation_alpha} shows the search results on the same MM problem with a set of different $\alpha$ values. We see that the evolutionary search converges fastest and to the best results when $\alpha=0.4$. 


% \begin{figure}[t]
% \includegraphics[width=\columnwidth]{figs/crossover_example.png}
% \centering
% \caption{Example of crossover in evolutionary search.}
% \label{fig:crossover_example}
% \end{figure}

\textbf{Crossover}
The crossover operation exchanges the genomes of two individuals. To guarantee the validness of the offspring, we exchange the tiling factors associated with the same original loop together. %Figure~\ref{fig:crossover_example} shows such an example.

%With evolutionary search, Odyssey could converge to designs achieving 90\% of the optimal performance in \todo{...} seconds with a single CPU thread. To further improve the convergence speed, we introduce a MP-based optimizer to provide a high-quality initial input to evolutionary search. With this method, w

\subsubsection{Mathematical Programming}
\label{sec:solver_details}
%The previous work ConfuciuX~\cite{confuciux} has pointed out that the convergence of evolutionary search can be improved with a high-quality initial population. 
Although the mathematical programming (MP)-based method fails to identify the optimal design, the design it finds achieves relatively good performance and could be used as the initial population of evolutionary search. Odyssey implements a MP-based optimizer to produce high-quality seeds to evolutionary search. We formulate the optimization problem as follows.

\textbf{Objective Function:} Given the high complexity of the hardware designs, it is usually hard to have a close-formed performance model suitable for the solvers as the objective function. Instead, previous works chose different high-order functions that impact the performance as the objective functions~\cite{analytical_cnn,marvel,cosa}. 
%The recent work CoSA~\cite{cosa} proposed using a linear combination of three high-order functions including the memory resource utilization, ideal compute latency, and the on-chip data traffic. However, the coefficients of these factors need to be specified by users which adds unnecessary complexity to the tuning process. 
We conducted an experiment on evaluating the effectiveness of several objective functions.

\textbf{Objective 1: Computation resource} We use the total DSP usage $U_{DSP}$ as the optimization target. The heuristic is that a design with higher performance should utilize more DSPs.
\begin{equation}
\small
Obj1: min(-U_{DSP})
\vspace{-0.025in}
\end{equation}
\textbf{Objective 2: Off-chip communication} 
We aggregate the off-chip data movement of all the arrays in the program. For communication-bound applications, reducing the off-chip communication could improve the design performance. 

\begin{equation}
\small
Obj2: min\sum_{a\in Arrays} DM(a)
\vspace{-0.025in}
\end{equation}

%\paragraph{Objective 3: Off-chip communication / computation resource} This objective %function takes both computation and communication into consideration. Ideally, we would %like to maximize the computation resource and reduce the off-chip communication.
%
%\begin{equation}
%Obj3: min(\sum_{a\in Arrays} DM(a)/U_{DSP})
%\end{equation}

\textbf{Objective 3: Off-chip communication - computation resource} 
This objective function takes both computation and communication into consideration. Ideally, we would like to maximize the computation resource and reduce the off-chip communication.
%This objective function is similar to the previous one with the division relation replaced by subtraction. 

\begin{equation}
\small
Obj3: min(\sum_{a\in Arrays} DM(a)-U_{DSP})
\vspace{-0.025in}
\end{equation}

\textbf{Constraints:} A valid hardware design should not overuse the available memory and computation resource. For FPGA designs, we consider the BRAM and DSP usage.
\begin{equation}
\small
U_{mem} \leq Mem_{available},\,\,U_{DSP} \leq DSP_{available}
\vspace{-0.025in}
\end{equation}

Where $U_{DSP}$ and $U_{mem}$ are the sum of the DSP and memory usage of each hardware module in the systolic array.

% \begin{equation}
% \small
% U_{mem} = \sum_{m\in Modules}{U_{mem}(m)\times N_m}
% \vspace{-0.025in}
% \end{equation}


% which is a sum of the memory usage of each type of hardware module $m$, computed by multiplying the memory usage of module $m$ ($U_{mem}(m)$) with the total number of such modules $N_m$.
%The memory usage of module $m$ is computed as:
%\begin{equation}
%\small
%U_{mem}(m) = \sum_{a\in Arrays}{\lceil{\frac{DW(a)\times %Bank(a)}{18}\rceil}\lceil{\frac{N(a)}{Bank(a)\times 1024}}\rceil}
%\end{equation}
%
%We sum the BRAMs used by all the arrays inside the hardware module. On Xilinx FPGA, the basic %BRAM unit is configured with a datawidth of 18 bits and a depth of 1024 words. The datawidth %of the on-chip buffer for storing the array $a$ is computed as the array data width $DW(a)$ %multiplied by the number of memory banks $Bank(a)$. The depth of the buffer is computed as %the total number of array elements $N(a)$ divided by the number of memory banks $Bank(a)$.
%The total number of the BRAM units should not surpass the maximal available memory resource on-chip $Mem_{available}$.

% \paragraph{Computation resource}
% %Similar to the memory resource, the total usage of computation resources (i.e., DSP) should not go beyond the limit on-chip.
% We compute the DSP usage as:
% \begin{align}
% \small
% U_{DSP} &= \sum_{m\in Modules}{U_{DSP}(m)\times N_m} \\
% U_{DSP}(m) &= \sum_{op \in Compute\_Ops}{SIMD(op)\times U_{DSP}(op)}
% \vspace{-0.025in}
% \end{align}

% We calculate the total amount of DSPs as a sum of the DSPs consumed by all the hardware modules and the per module DSP usage $U_{DSP}(m)$ is computed as a sum of the products of SIMD factor $SIMD(op)$ of each computation operator $op$ and the number of DSPs consumed by each single SIMD lane $U_{DSP}(op)$. We maintain an internal database for the per operator DSP usage $U_{DSP}(op)$.

%\paragraph{Architecture constraints}
%In addition to the resource constraints, there are two other additional constraints we %need to consider.
%
%\textit{Latency hiding}. To achieve 100\% of the pipeline utilization with latency %hiding, we require the product of the loop bounds of all the latency hiding loops %$T(l)$ to be no less than the critical path latency of the computation statements %$L_{compute\_critical}$.
%\begin{equation}
%\prod_{l\in latency\_hiding\_loops}T(l) \geq L_{compute\_critical}
%\end{equation}
%
%We maintain an internal database storing the latency of different computation %statements and accumulate them along the critical path to estimate %$L_{compute\_critical}$.
%
%\textit{SIMD vectorization}. We require the SIMD factor to be an even number to %minimize the hardware overheads.


We use the off-the-shelf solver (AMPL~\cite{ampl} with Ipopt~\cite{ipopt}) to implement the optimization problem. All the metrics have been normalized. The best solution obtained from the solver is then fed to the evolutionary search as the initial population. Figure~\ref{fig:tuning_obj} shows the search traces of evolutionary search with different optimization targets. Table~\ref{table:tuning_obj_cmp} compares the performance of designs found by solvers.
%DSP usage is normalized against the total available DSP resource on-chip. For data movement, we compute a baseline by adding up the total amount of elements in each array. For arrays that are associated with flow dependences (e.g., the output matrix $C$ in matrix multiplication), we multiply the total amount of elements by 2 in consideration of both inbound and outbound traffic. The data movement $DM(a)$ is normalized against this baseline in all the optimization problems.

\begin{figure}[h]
\includegraphics[width=0.7\columnwidth]{figs/solver_cmp.png}
\centering
\vspace{-0.05in}
\caption{Search traces of evolutionary search initiated with designs found by the MP-based optimizer with different objective functions.}
\vspace{-0.05in}
\label{fig:tuning_obj}
\end{figure}

%\todo{fix the figure, reduce the opt targets to only 3}
As seen in the figure, all three objective functions help reduce the convergence time and yield better results compared to the evolutionary search-only method (annotated as \textit{No Solver}). Specifically, Obj3 helps significantly reduce the convergence time. With Obj3, the auto-tuner locates a better design than the baseline (No Solver) within 2000 epochs. Thus, we use the Obj3 as the optimization target of the solver. 

% First, MP-based methods are insufficient to find optimal designs. As demonstrated in Table~\ref{table:tuning_obj_cmp}, the best design identified by the MP-based approach is 1.5$\times$ slower than the design discovered by Odyssey with a hybrid search method that combines both mathematical programming and evolutionary search.

%as the first epoch of the evolutionary search starts with the sample design generated from the analytical modeling. It can be clearly observed that neither of the optimization targets yields designs with optimal performance. Table~\ref{table:tuning_obj_cmp} shows the details of the designs found by each approach. The best design found by the analytical modeling-based approach is 2.0$\times$ slower than the best design found by Odyssey. This result reveals the limitations of analytical modeling-based methods that usually lead to inferior designs with the over-simplified design space formulation. 
\begin{table}[h]
\centering
\vspace{-0.05in}
\caption{Comparison of the best solutions found by MP-based methods and Odyssey framework.}
\resizebox{\columnwidth}{!}{
%\begin{tabular}{ccccccccccc}
%\toprule
%Methods & Opt. Target & Latency & Off-Chip Data Movement & DSP  & T\_{I1} & T\_{J1} & T\_{K1} & T\_{I2} & T\_{J2} %& T\_{K2} \\ \toprule
%Solver  & Obj1:-comp       & 1.5$\times$ & 1.7$\times$   & 6.3$\times$    & 520   & 520   & 320   & 26    & 26    %& 4     \\ \hline
%Solver  & Obj2:comm        & 8.6$\times$ & 1.0$\times$   & 1.0$\times$     & 1024  & 1024  & 320    & 128   & 128 %  & 4     \\ \hline
%Solver  & Obj3:comm-comp   & 2.5$\times$ & 1.0$\times$   & 4.0$\times$    & 1024  & 1024  & 320    & 64    & 64   % & 4     \\ \hline
%Odyssey    & comm-comp   & 1.0$\times$  & 4.9$\times$       & 6.7$\times$    & 129   & 130   & 64    & 3    & 13  %   & 4     \\ \bottomrule
%\end{tabular}
\begin{tabular}{ccccc}
\toprule
Methods & Opt. Target & Latency & Off-Chip Data Movement & DSP   \\ \toprule
Mathematical Programming  & Obj1:-comp       & 1.5$\times$ & 1.7$\times$   & 6.3$\times$ \\ \hline
Mathematical Programming  & Obj2:comm        & 8.6$\times$ & 1.0$\times$   & 1.0$\times$ \\ \hline
Mathematical Programming  & Obj3:comm-comp   & 2.5$\times$ & 1.0$\times$   & 4.0$\times$ \\ \hline
Odyssey    & comm-comp   & 1.0$\times$  & 4.9$\times$       & 6.7$\times$  \\ \bottomrule
\end{tabular}
}
\label{table:tuning_obj_cmp}
% \vspace{-0.2in}
\end{table}

% Second, the optimal design does not necessarily minimize the off-chip data communication.

% In Table~\ref{table:tuning_obj_cmp}, the best design found by Odyssey introduces 4.9$\times$ more off-chip data communication than designs identified by Obj2 and Obj3. 
%all the analytical modeling-based approaches except Obj1 achieve the least off-chip data movement among all designs. However, this does not directly lead to the best performance. 
%Without the consideration of computation, Obj2 leads to a design which is 3.4$\times$ slower than Obj3 that balance communication and computation. 
%The best design found by Odyssey introduces 2.4$\times$ more off-chip data communication than the design with the least off-chip data movement. 
%This indicates that previous works such as Marvel~\cite{marvel} and Chen et al.~\cite{chen_communication} that chose to decouple the design space by first finding the subspace that minimizes the off-chip data communication will leave out the optimal designs and achieve inferior performance.

%Lastly, our search results echo the findings from ConfuciuX~\cite{confuciux} that the performance of evolutionary search can be improved with a high-quality initial population. Based on the experiment above, we choose Obj3 as the optimization target of the static analyzer.


\subsection{Padding-based Algorithm}
In Section~\ref{sec:intro}, we showed that limiting the tiling factors to the divisors of the problem size can lead to inferior designs. The reason is that sometimes divisor tiling factors do not allow for full utilization of the available resources such as DSPs, BRAMs, or bandwidth. Thus, considering non-divisor tiling factors can help maximize some or all of these resources. Also, note that when tiling factors are non-divisors, the original problem size needs to be padded with zeros to yield integer values for the outer loops' trip counts. Therefore, considering non-divisor tiling factors is equivalent to first finding a set of the possible zero-padded problem sizes (Lines 9-11 of the algorithm), and then \textit{\textbf{only}} searching for the divisor tiling factors of each zero-padded problem size. 

\textbf{Heuristic:} \textit{The main idea of our algorithm is that since zero padding adds computation and communication overheads, we should explore the design space in the order of increasing the zero-padding.}



% Since zero padding adds computation and communication overheads, we can now perform the search in the order of increasing the zero-padding. 

% Since we know that zero padding adds computation and communication overheads, and after certain amount of zero-padding, the overhead is more than the gain we achieve from using non-divisor tiling factor

% The main idea/heuristic of our padding-based algorithm is based on three observations: 1) When tiling factors are non-divisors, the original problem size needs to be padded with zeros to yield integer values for the outer loops' trip counts. 2) Zero padding adds some computation and communication overheads. 3) We know that after certain amount of zero-padding, the overhead is more than the gain we achieve from using non-divisor tiling factor.

% Now, let us take $64\times64\times64$ MM for example. For the dimension $I$, if we consider all the tiling factors $T_{i1} \in [1, 2, 3, 4, 5, ... 64]$, the new padded dimension would be given by $\ceil*{I/T_{i1}}\times T_{i1}$ giving us the following padded dimensions $[64, 64, 66, 64, 65, ... 64]$. We observe that the list of padded dimensions is not sorted in increasing order. Moreover, it is know that zero-padding adds some overhead.
% If we perform an exhaustive search considering all the 

% It is know that there is an inverse relationship between performance and zero padding, but we just showed that in some cases the zero padding coming from using non-divisor tiling factors can help find better designs.

%  For each dimension, the divisor tiling factors are $[1, 2, 4, 8, 16, 32, 64]$. 
% As discussed earlier, limiting the design space to divisor-only tiling factors can miss the optimal designs as these factors sometimes cannot utilize all the DSPs, BRAMs or any other constraint.  The main idea/heuristic behind our algorithm is that non-divisor tiling factors can help maximize one of the design constraints but at the cost of adding some zero-padding overhead. If we rearrange the search to start by incremently adding zero-padding 
\begin{algorithm}
\caption{Padding-based Algorithm for MM}
\footnotesize
\DontPrintSemicolon
\SetKwFunction{getPaddedCandidates}{padded\_candidates}
\SetKwProg{Fn}{Function}{:}{}
\Fn{\getPaddedCandidates{$dim$}}{
    $padded\_candidates\_set \gets \texttt{new}~set$\;
    \For{$i$ {\color{blue}in} range($dim$)}{
        $padded\_size \gets \ceil*{\frac{dim}{i}}\times i$\;
        $padded\_candidates\_set.append(padded\_size)$
    }
    $padded\_candidates\_set.sort()$\;
    \Return $padded\_candidates\_set$\;
}
\SetKwFunction{FSearch}{Search}
\SetKwProg{Fn}{Function}{:}{}
\Fn{\FSearch{$I$, $J$, $K$, $i\_thr$,$j\_thr$,$k\_thr$}}{
    $padded\_i\_candidates \gets \texttt{padded\_candidates}(I)$\;
    $padded\_j\_candidates \gets \texttt{padded\_candidates}(J)$\;
    $padded\_k\_candidates \gets \texttt{padded\_candidates}(K)$\;
    $i\_counter,~j\_counter,~k\_counter \gets 0,~0,~0$\;
    $best\_result \gets \{cycles: \infty,~params:\texttt{None}\}$\;
    \For{$i$ {\color{blue}in} $padded\_i\_candidates$}{
        $i\_updated \gets False$\;
        \For{$j$ {\color{blue}in} $padded\_j\_candidates$}{
            $j\_updated \gets False$\;
            \For{$k$ {\color{blue}in} $padded\_k\_candidates$}{
                $k\_updated \gets False$\;
                $result \gets\texttt{search\_divisors\_only}(i, j, k)$\;
                \If{$result.cycles < top\_result.cycles$}{
                    $top\_result \gets result$\;
                    $k\_updated \gets True$\;
                    $j\_updated \gets True$\;
                    $i\_updated \gets True$\;
                    $k\_counter \gets 0$\;
                }\Else{
                    $k\_counter~+= 1$\;
                }
                \If{$k\_counter > k\_thr$}{
                    $k\_counter \gets 0$\;
                    \textbf{break}\;
                }
            }
            \If{$j\_updated$}{
                $j\_counter \gets 0$\;
            }\Else{
                $j\_counter~+= 1$\;
            }
            \If{$j\_counter > j\_thr$}{
                $j\_counter \gets 0$\;
                \textbf{break}\;
            }
        }
        \If{$i\_updated$}{
            $i\_counter \gets 0$\;
        }\Else{
            $i\_counter~+= 1$\;
        }
        \If{$i\_counter > i\_thr$}{
            \textbf{break}\;
        }
    }
    \Return $best\_result$\;
}

\end{algorithm}

An example of the padding-based algorithm for the MM case is shown in Algorithm 1. The same algorithm is used for convolution but with more loops and is omitted for space.

\textbf{Termination Criteria:}
A naive termination criteria for the algorithm would be to search a fixed $N$ padding candidates on each dimension. However, this approach can miss some optimal points if they are beyond the $N$th candidate. A better termination criteria would be to use adaptive counters with thresholds. The counters are reset to zero whenever a better design is found allowing the algorithm to explore more points in that direction, otherwise, if there is no improvement in $threshold$ consecutive candidates, it breaks. Empirically, we found that setting thresholds to be $\ceil*{0.5\sqrt{dim}}$ achieves the optimal solutions found by exhaustive search for most of the cases. 

% As a first step, the algorithm gets the sorted sets of the zero-padded problem sizes (Lines 9-11) using the \texttt{get\_padded\_candidates} function. 
\section{Design Space}

In this section, we define the design space of systolic arrays and the automatic generation of their accurate performance models.
% There is a plethora of previous works on performance tuning of systolic arrays~\cite{timeloop,confuciux,cosa,dmazerunner,marvel,tenet,interstellar,gamma,hegde2021mind}. Table~\ref{table:prior_work_tuning} lists several recent works. Note that some of the listed works covered a broader set of architectures beyond systolic arrays. In this work, we only focus on the systolic array architecture. However, the methodology proposed in this work can be applied to other architectures as well. We discuss the prior works from two dimensions: the design space and search methods.

\subsection{Automatic Systolic Array Generation}
\label{sec:systolic_generate}
Automatic systolic array generation is an important research topic given the high performance of the systolic array architecture and the complexities of the designing process~\cite{gemmini,susy,calyx}. The recent work, AutoSA~\cite{autosa}, reported the best performance results in this field.
% ,mmalpha,uday_ppopp
%Figure \todo{...} presents the compilation flow of AutoSA. 
% mmalpha,uday_ppopp,calyx
AutoSA takes in a C program as the input and applies a sequence of program transformations on this program to build and optimize systolic arrays.

With a comprehensive coverage of hardware optimization techniques, AutoSA generates high-performance systolic arrays with comparable or better performance than previous works~\cite{autosa}. However, such a vast design also poses significant challenges to performance tuning. As an example, considering all the available tuning options, the size of design space bloats to $O(2^{40})$ for a $1024\times 1024\times 1024$ MM. 
This challenge has motivated us to develop Odyssey which provides efficient auto-tuning support to explore such a design space.

% The subfigure at the bottom-right of Figure~\ref{fig:autosa_flow} summarizes the design space covered by Odyssey. Odyssey explores different dataflows in the space-time transformation and the loop tiling factors in array partitioning, latency hiding, and SIMD vectorization. Odyssey also explores non-divisor tiling factors and loop permutation in the step of array partitioning. When a non-divisor tiling factor is chosen, the original problem dimension is zero padded to be a multiple of the tiling factor. 
% Tiling factors of latency hiding and SIMD vectorization are still required to be divisors as the array dimension is fixed after array partitioning and non-divisor tiling factors at later steps introduce prologue/epilogue phases and costly max/min operations that hurt the design performance. 


\begin{figure}[t]
\includegraphics[width=0.8\columnwidth]{figs/autosa_space.pdf}
\centering
%\vspace{-0.05in}
\caption{Example tuning space for Matrix Multiplication}
\label{fig:autosa_flow}
\end{figure}

\renewcommand{\arraystretch}{1.2} 
\begin{table}[t]
\centering
\caption{Parameters used to construct the design space for MM and Conv}
\vspace{-0.05in}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc}
\toprule
Application      & MM                                                                                                                & CNN                                                                                                                                             \\ \hline
Dataflows        & {[}i{]}, {[}j{]}, {[}k{]}, {[}i,j{]}, {[}i,k{]}, {[}j,k{]}                                                        & \begin{tabular}[c]{@{}c@{}}{[}o{]}, {[}h{]}, {[}w{]}, {[}i{]}, \\ {[}o,h{]}, {[}o,w{]}, {[}o,i{]}, {[}h,w{]}, {[}h,i{]}, {[}w,i{]}\end{tabular} \\ \hline
Loop Permutation & \textless{}{[}i,j{]},k\textgreater{}, \textless{}{[}j,k{]},i\textgreater{}, \textless{}{[}i,k{]}, j\textgreater{} & \textless{}{[}o,h,w{]},{[}i,p,q{]}\textgreater{}, \textless{}{[}o,i,p,q{]},{[}h,w{]}\textgreater{}, \textless{}{[}i,h,w,p,q{]},o\textgreater{}  \\ \hline
\# of Unique Designs        & 6 dataflows $\times$ 3 permutations $=18$                                                                                                               & 10 dataflows $\times$ 3 permutations $=30$ \\ \hline         
Tiling Factors per Design      & 
$T_{i1}, T_{j1}, T_{k1}, T_{i2}, T_{j2}, T_{k2}$ &
$T_{i1}, T_{o1}, T_{h1}, T_{w1}, T_{i2}, T_{o2}, T_{h2}, T_{w2}$     \\ \hline
\end{tabular}
}
\vspace{-0.2in}
\label{table:tuning_space}
\end{table}

\subsection{Constructing the Design Space} We consider three dimensions of the design space: dataflows, loop permutation and loop tiling. Figure~\ref{fig:autosa_flow} shows an example of the design space for a matrix multiplication kernel. Table~\ref{table:tuning_space} shows the parameters used to construct the whole design space for MM and convolution kernels.
% In addition, in this work, we cover non-divisor tiling factors in loop tiling. Many previous works only used divisors for simplicity~\cite{timeloop,dmazerunner,interstellar,marvel,cosa}. However, this choice could lead to significant performance loss, as discussed in Section~\ref{sec:intro}.
\subsubsection{Dataflows}
Previous works~\cite{eyeriss,interstellar,maestro} used the term \textit{dataflow} to identify different array topologies and execution patterns, which are equivalent to different space-time mappings within the scope of systolic arrays. %different dataflows are equivalent to different space-time mappings that determine the unique array topology and execution pattern. 
In the rest of the paper, we use \textit{dataflow} to refer to different space-time mappings. We annotate each dataflow in the format of $[i,j]$ that marks the selected space loops for this array (see Table~\ref{table:tuning_space}).

\subsubsection{Loop Permutation}
\label{sec:loop_permute}
Odyssey explores different loop permutation orderings in the array partitioning. Different loop orderings may lead to various array architectures. AutoSA enumerates all the loop permutation orderings. For MM, there are $3!=6$ different orderings to consider. The number grows larger for a more complicated applications like 2-D convolution. The six-level nested loop leads to $6!=720$ different loop orderings. However, as pointed out by previous works~\cite{analytical_cnn,analytical_tc,dmazerunner}, among all the loop orderings, many of them are dominated by a few orderings in performance, thus can be safely pruned down to three unique permutations without leaving out the optimal points. Accordingly, the number of unique systolic arrays for MM and convolution is depicted in the third row of Table~\ref{table:tuning_space}.
% the number of unique systolic array architectures is 6 dataflows $\times$ 3 loop permutations $=18$ systolic arrays for MM and 10 dataflows $\times$ 3 loop permutations $=30$ unique systolic arrays for convolution as shown in Table~\ref{table:tuning_space}.
% Listing~\ref{lst:cnncode} shows the example code of one CNN layer.
% Next, we show that with proper pruning, we can reduce the number of loop orderings to consider for both MM and CNN to only 3. 

\subsubsection{Loop Tiling}
For a given unique dataflow and loop permutation, AutoSA generates two-level tiling for each dimension of the problem size. The first-level tiling factors are for the array partitioning, and the second-level tiling factors are for latency hiding and SIMD vectorization (refer to AutoSA~\cite{autosa} for more details). In addition, in this work, we cover non-divisor tiling factors in the first-level loop tiling. Many previous works only used divisors for simplicity~\cite{timeloop,dmazerunner,interstellar,marvel,cosa}. However, this choice could lead to significant performance loss, as discussed in Section~\ref{sec:intro}.



\subsection{Automatic Performance Model Generation}
The accuracy of performance models plays an important role in performance tuning. In Section~\ref{sec:intro}, we discussed the issue of using a simplified performance model that overlooks the epilogue and prologue phases of the hardware execution. %which obfuscates the search process and leads to inferior performance. 
Table~\ref{table:prior_work_tuning} highlights several previous works with a similar issue~\cite{timeloop,dmazerunner,tenet}. In addition, such performance models are usually derived manually which is time-consuming and error-prone. Odyssey distinguishes itself from the prior works in that it automatically creates performance models by leveraging the AutoSA compiler. 
We have extended AutoSA to generate a design descriptor that contains all the necessary information for estimating the design performance.

The auto-tuner utilizes this description file to create a Python file containing functions for estimating the design performance. All the performance models are symbolic expressions of the tuning parameters.
During the search, the auto-tuner samples the design space and plugs in different tuning parameters into the performance models for assessing the design performance. 



\begin{table}[h]
\caption{Comparison between different systolic array architecture performance tuning frameworks.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
                                 & Design Space             & \multicolumn{2}{c}{Performance Models}                                                     & \multirow{2}{*}{Search Methods}                                                     \\ \cline{1-4}
                                 & Non-Divisors             & \begin{tabular}[c]{@{}c@{}}Prologue/\\ Epilogue\end{tabular} & Generation                  &                                                                                     \\ \toprule
Timeloop~\cite{timeloop}         & \textcolor{red}{N}       & \textcolor{red}{N}                                           & \textcolor{red}{Manual}     & \begin{tabular}[c]{@{}c@{}}Exhaustive w/ Pruning\\ Random Search\end{tabular}       \\ \hline
dMazeRunner~\cite{dmazerunner}   & \textcolor{red}{N}       & \textcolor{red}{N}                                           & \textcolor{red}{Manual}     & Exhaustive w/ Pruning                                                               \\ \hline
Interstellar~\cite{interstellar} & \textcolor{red}{N}       & N/A                                                          & \textcolor{red}{Manual}     & Exhaustive w/ Pruning                                                               \\ \hline
Marvel~\cite{marvel}             & \textcolor{red}{N}       & {\color{green!55!blue}Y}                                     & \textcolor{red}{Manual}     & \begin{tabular}[c]{@{}c@{}}Mathematical Programming\\ Exhaustive w/ Pruning\end{tabular} \\ \hline
ConfuciuX~\cite{confuciux}       & N/A                      & {\color{green!55!blue}Y}                                     & \textcolor{red}{Manual}     & \begin{tabular}[c]{@{}c@{}}RL\\ Evolutionary Search\end{tabular}                    \\ \hline
CoSA~\cite{cosa}                 & \textcolor{red}{N}       & {\color{green!55!blue}Y}                                     & \textcolor{red}{Manual}     & Mathematical Programming                                 \\ \hline
TENET~\cite{tenet}               & N/A                      & \textcolor{red}{N}                                           & \textcolor{red}{Manual}     & Exhaustive w/ Pruning                                                               \\ \hline
\textbf{Odyssey}                          & {\color{green!55!blue}Y} & {\color{green!55!blue}Y}                                     & {\color{green!55!blue}Auto} & \begin{tabular}[c]{@{}c@{}}Mathematical Programming\\ Evolutionary Search\end{tabular}   \\ \bottomrule
\end{tabular}
}
\vspace{-0.2in}
\label{table:prior_work_tuning}
\end{table}
% \subsubsection{Performance model} 


\section{Evaluation Results}

First, we evaluate the performance and sample efficiency of our genetic-MP hybrid method against prior iterative and random search methods. Since our work targets the design space of systolic array while the previous works targeted different design spaces, it is difficult to conduct direct comparisons. Thus, we use the search methods adopted in these works and compare them with Odyssey. 
Next, we compare the performance of the padding-based algorithm.
Finally, we validate our findings by implementing two systolic arrays for $1024\times 1024\times 1024$ MM.
% \begin{figure*}[!h]
% \includegraphics[width=2\columnwidth]{figs/mm_throughput_speedup.pdf}
% \includegraphics[width=2\columnwidth]{figs/conv_throughput_speedup.pdf}
% \centering
% \caption{Normalized throughput comparisons for 10 MM kernels and 16 convolution layers}
% \label{fig:tuning_time_limit}
% \end{figure*}


\subsubsection{Genetic-MP Hyprid Method}
~\\
\textbf{Workloads:} We evaluate the performance of the genetic-MP hybrid method using a $1024\times1024\times1024$ MM. 

\textbf{Baselines:} We use the following methods as baselines.


    1) \textit{Random search}. We randomly sample the design space and update the best solutions. 
    
    %We also maintain the history of searched solutions to avoid duplicate sampling.
    2) \textit{Exhaustive search with pruning}. We extend the random search by pruning the design samples based on the DSP utilization. 
    As the smallest design among the 18 designs uses 30\% of DSPs, we set the DSP pruning threshold to 25\%.

    3) \textit{Simulated annealing}. 
    We use the Python package~\cite{python_annealing} as the baseline. 
    Based on a grid search, we designate the hyper-parameter temperature $T$ to be 200.
    We implement a customized step-taking function using the proposed hybrid mutation method for evolutionary search.
    
    4) \textit{Bayesian optimization}. 
    We use the Python package~\cite{python_bayesian} as the baseline. 

    5) \textit{OpenTuner~\cite{opentuner}}. OpenTuner is an auto-tuning framework built on an ensemble of several efficient search techniques. 
    We use the latest release of OpenTuner from its Github repository~\cite{opentuner_github}.
    % It has been demonstrated effective in cases such as searching the GCC compilation flags and optimal schedules for Halide programs~\cite{opentuner}.
    
    6) \textit{Reinforcement learning (RL)}. RL is a machine learning algorithm that can be used for hardware optimization.
    %which is consisted of an \textit{agent} taking a sequence of \textit{actions} based on the rewards or penalties from \textit{environment} using a \textit{policy}.
    %Depending on the actions that the agent makes, the environment generates either rewards or penalties to the agent. The goal to is maximize the total \textit{rewards} in a limited number of actions. The agent implements a \textit{policy} to determine the next action given all the past actions and feedback from the environment. 
    %RL has been used on many application domains including neural architecture search~\cite{zoph2016neural} and hardware design space %exploration~\cite{confuciux,shi_hardware}.
%    %For example, 
    The previous work ConfuciuX~\cite{confuciux} implemented a two-step search algorithm for tuning the dataflow architectures which employs RL as the first step to locate a good sub design space and utilizes evolutionary search to perform a more fine-grained search later to find the best design. We use the open-source implementation from ConfuciuX as the RL baseline~\cite{confuciux_github}. ConfuciuX applied a 3-layer multi-layer perceptron (MLP) neural network for the policy network.
% \end{itemize}

\textbf{Designs:} We compare our tuning methods against the baselines on all 18 different designs generated for MM by AutoSA. 

\textbf{Evaluation method:} For each systolic array design, we run the search method for 5 minutes and repeat it 3 times. 
The final results are averaged from the 3 runs. All the search methods are executed with a single CPU thread. RL baseline uses Pytorch which implements multi-threading during the training of MLP. All experiments are executed on a workstation with Intel Xeon E5-2680 v4 CPU.

\begin{figure}[h]
\includegraphics[width=0.9\columnwidth]{figs/cmp_methods_all.png}
\centering
\caption{Comparison of the best designs found by different tuning methods considering all dataflows and loop permutations in Table~\ref{table:tuning_space}}
\label{fig:tuning_methods_all_cmp}
\end{figure}


\textbf{Search results quality:}
Figure~\ref{fig:tuning_methods_all_cmp} compares the best throughput (1/latency) achieved by each tuning method on the 18 systolic array designs. The throughput is normalized against the optimal performance found by exhaustive search\footnote{We run an exhaustive search until it finishes.}.
%Table~\ref{table:tuning_method_cmp} summarizes the best design and convergence time\footnote{Convergence time is measured as the last time when the search results are updated.} of all the search methods on the 18 systolic array designs. The design latency of all the methods are normalized against the optimal latency found by exhaustive search.
%\footnote{As exhaustive search takes too long and is infeasible for this task, we apply the DSP pruning and adjust the DSP pruning threshold dynamically for each design based on the best design found by other methods. The final results are collected by running exhaustive search with pruning until it completes.}. 
%The best result found for each design is marked in green color. %
The genetic-MP method found design configurations with best performance on 13 designs out of the total 18 designs. 
For the remaining 5 designs, the performance gap is within 1\% of the best performance identified by other baselines (OpenTuner and simulated annealing). 
Overall, genetic-MP method locates designs that achieve more than 95\% of the optimal performance. %\todo{fix this figure}

\begin{figure}[h]
\includegraphics[width=0.9\columnwidth]{figs/converge_cmp.png}
\centering
\vspace{-0.025in}
\caption{Comparison of sample efficiency of different tuning methods.}
\vspace{-0.025in}
\label{fig:tuning_methods_epoch}
\end{figure}

\textbf{Sample efficiency:}
In addition to the high-quality search results, the genetic-MP method achieves high sample efficiency. Figure~\ref{fig:tuning_methods_epoch} compares the convergence traces of all the tuning methods on the design with the highest optimal throughput. As shown by the figure, Odyssey finds a good design configuration resulting in 93\% of the optimal performance after evaluating 3000 design samples. Simulated annealing earns the second best performance, locating a design that reaches 66\% of the optimal performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{figs/all_throughput_speedup.pdf}
    \caption{Normalized throughput comparisons for 4 MM kernels and 4 convolution layers}
    \label{fig:padding_results}
\end{figure}
\label{sec:evaluate_results}

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccc}
    \multirow{2}{*}{Design} & \multicolumn{2}{c}{Exhaustive Search} & \multicolumn{2}{c}{Padding-Based Algorithm} & Design Space & Runtime \\ \cline{2-3} \cline{4-5}
                                 & \# of Designs & Runtime & \# of Designs & Runtime & Reduction & Speedup \\ 
                                 &               & (hours) &               & (hours) &           & \\ \hline
    K0 & 3.66B & 1.59 & 28.64M & 0.06 & $127.7\times$ & $25.3\times$ \\
    K1 & 7.75B & 3.24 & 41.27M & 0.09 & $187.7\times$ & $36.7\times$ \\
    K2 & 33.17B & 13.83 & 105.99M & 0.19 & $312.9\times$ & $73.1\times$ \\
    K4 & 34.80B & 14.71 & 198.17M & 0.19 & $175.6\times$ & $78.6\times$ \\
    K5 & 53.06B & 26.44 & 620.03M & 0.54 & $85.6\times$ & $49.0\times$ \\
    K6 & 84.68B & 34.56 & 356.91M & 0.28 & $237.3\times$ & $122.1\times$ \\
    \end{tabular}
    }
    \caption{Comparing exhaustive search and padding-based algorithm in terms of explored design space and runtime}
    \label{tab:comp_runtime}
\end{table}

\subsection{Padding-based Algorithm}
We compare the padding-based against the divisor-only exhaustive search, the complete exhaustive search (including divisors), and the genetic-MP hybrid method in terms of performance and runtime. We used 4 MM kernels (K0 - K3) and 4 convolution layers from four famous CNNs (K4 - K7). We used the complete exhaustive search as a baseline, except for K3 and K7 as these kernels have huge design spaces with more than 200 billion points. Figure~\ref{fig:padding_results} shows the best performance found by each search method. Using a threshold of $\ceil*{0.5\sqrt{dim}}$, the padding-based algorithm finds the optimal points for 5 out of the 6 kernels whose exhaustive search completed. For K6, increasing the threshold to  $\ceil*{0.6\sqrt{dim}}$ finds the optimal point as well. The designs found by the genetic-MP method are within less than 11\% of the best designs.

Table~\ref{tab:comp_runtime} shows the design space reduction and runtime speedups of the padding-based algorithm compared to the complete exhaustive search. Both search methods are run using 36 threads on Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz. 
% Figure~\ref{fig:tuning_time_limit} shows the results of our two search methods compared with a pruned exhaustive search considering divisor tiling factors only as well as a complete exhaustive search considering both divisor and non-divisor tiling factors whenever it is feasible\footnote{For large MM and conv problem sizes, the design space can be of size $O(2^{40})$}. 
% As shown in Figure~\ref{fig:tuning_time_limit}, the padding-based algorithm finds the same results of the exhaustive search for all the cases where the exhaustive search is feasible. For all the tests, the number of points our algorithm checks is less than 2\% of the whole design space. In terms of the run-time, the algorithm takes between $11\times$ and $122\times$ less time than the exhaustive search. Taking the layer $(I=64, O=384, R=14, C=14, P=1, Q=1)$ from MobileNet-V2, the complete exhaustive method explores 21 billion designs taking 11.7 hours while our algorithm finds the same optimal point by exploring 100 million designs in 10 minutes.




\subsection{Validation Results}
To validate the results of this work, we implemented two systolic arrays on FPGA for $1024 \times 1024 \times 1024$ MM. The first design is the best design found by the exhaustive search considering divisor tiling factors only. The second design is found by Odyssey considering non-divisor tiling factors. The on-board results match our analysis in Section~\ref{sec:intro}, and the non-divisor systolic array delivers $1.72\times$ throughput improvement.
\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccccc}
        \multirow{2}{*}{Search Method} & Padded Problem Size & SA Size               & \multirow{2}{*}{DSPs}     & \multirow{2}{*}{BRAMs} & Frequency & Throughput  & \multirow{2}{*}{Speedup} \\ 
        & $(I, J, K)$         & $(Cols, Rows, SIMD)$        &                           &                        &  (MHz)    & GFLOP/s     & \\ \hline  
        Divisor only & (1024, 1024, 1024)  & (32, 4, 8)     & 5133                      & 2543                   & 257       & 506.71      & $1\times$\\
        Odyssey & (1032, 1040, 1024)  & (43, 5, 8)          & 9258                      & 2932                   & 279       & 869.97      & $1.72\times$\\ \hline

    \end{tabular}
    }
    \caption{FPGA implementations of the best designs found by two search methods}
    \label{tab:my_label}
    \vspace{-0.2in}
\end{table}
